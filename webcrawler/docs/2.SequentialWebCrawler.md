# SequentialWebCrawler

## SequentialWebCrawler class
The SequentialWebCrawler class has several fields that are set via dependency injection: clock, parserFactory, timeout, popularWordCount, maxDepth, and ignoredUrls.

- clock field is used to track time
- parserFactory field is used to obtain a PageParser object, which is responsible for parsing the contents of a web page.
- timeout field determines how long the web crawler should run for, 
- popularWordCount and maxDepth fields determine how many popular words and how deep the crawler should navigate 
- ignoredUrls field is a list of URLs that should be ignored during the crawling process.

```java
@Inject
SequentialWebCrawler(
  Clock clock,
  PageParserFactory parserFactory,
  @Timeout Duration timeout,
  @PopularWordCount int popularWordCount,
  @MaxDepth int maxDepth,
  @IgnoredUrls List<Pattern> ignoredUrls) {
  this.clock = clock;
  this.parserFactory = parserFactory;
  this.timeout = timeout;
  this.popularWordCount = popularWordCount;
  this.maxDepth = maxDepth;
  this.ignoredUrls = ignoredUrls;
}
```

### crawl() method

The crawl() method is the entry point for the web crawler.

It takes a list of startingUrls, which are the URLs from which the crawler will begin navigating. The method initializes a set of visitedUrls and a map of counts, which will keep track of the word counts found during the crawling process. For each starting URL, the crawlInternal() method is called, passing in the URL, the deadline (determined by the timeout), the maximum depth, the counts map, and the visitedUrls set.

```java
@Override
public CrawlResult crawl(List<String> startingUrls) {
  Instant deadline = clock.instant().plus(timeout);
  Map<String, Integer> counts = new HashMap<>();
  Set<String> visitedUrls = new HashSet<>();
  for (String url : startingUrls) {
    crawlInternal(url, deadline, maxDepth, counts, visitedUrls);
  }

  if (counts.isEmpty()) {
    return new CrawlResult.Builder()
        .setWordCounts(counts)
        .setUrlsVisited(visitedUrls.size())
        .build();
  }

  return new CrawlResult.Builder()
      .setWordCounts(WordCounts.sort(counts, popularWordCount))
      .setUrlsVisited(visitedUrls.size())
      .build();
}
```

### crawlInternal() method
The crawlInternal() method is responsible for actually navigating to a URL and extracting the relevant information. It first checks if the maximum depth has been reached or if the deadline has passed, and if so, returns. Then, it checks if the URL matches any of the ignored patterns, and if so, returns.

Finally, it checks if the URL has already been visited, and if so, returns. If the URL passes all of these checks, it is added to the visitedUrls set and a PageParser object is obtained via the parserFactory. The PageParser object is then used to extract the word counts and links from the web page. The word counts are added to the counts map, while the links are recursively passed to crawlInternal() for further processing.

```java
  private void crawlInternal(
      String url,
      Instant deadline,
      int maxDepth,
      Map<String, Integer> counts,
      Set<String> visitedUrls) {
    if (maxDepth == 0 || clock.instant().isAfter(deadline)) {
      return;
    }
    for (Pattern pattern : ignoredUrls) {
      if (pattern.matcher(url).matches()) {
        return;
      }
    }
    if (visitedUrls.contains(url)) {
      return;
    }
    visitedUrls.add(url);
    PageParser.Result result = parserFactory.get(url).parse();
    for (Map.Entry<String, Integer> e : result.getWordCounts().entrySet()) {
      if (counts.containsKey(e.getKey())) {
        counts.put(e.getKey(), e.getValue() + counts.get(e.getKey()));
      } else {
        counts.put(e.getKey(), e.getValue());
      }
    }
    for (String link : result.getLinks()) {
      crawlInternal(link, deadline, maxDepth - 1, counts, visitedUrls);
    }
  }
```
